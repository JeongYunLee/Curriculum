---
title: 2. Classification

slug: 2-2

category: '좋은 분류Classification란'

 ---

### 2. 2. 좋은 분류Classification란: 성능 평가


머신러닝은 데이터 가공/변환, 모델 학습/예측, 그리고 평가evaluation의 순서로 진행됩니다. 모델이 잘 설명하는지를 알아보기 위해 평가를 진행합니다. 머신러닝은 여러 방법으로 예측 성능을 평가할 수 있습니다. 일반적으로 분류냐 회귀냐에 따라서 평가 기법이 나뉩니다. 


**회귀**의 경우 대부분 실제 값과 예측값의 오차 평균값에 기반합니다. 예를 들어 오차에 절댓값을 씌운  뒤 평균오차를 구하거나 오차 제곱값에 루트를 씌운 뒤 평균오차를 구하는 방법과 같이 기본적으로 예측오차를 가지고 정규화 수준을 재가공하는 방법이 회귀의 성능 평가 지표 유형입니다.


**분류** 또한 실제 결과 데이터와 예측 결과 데이터가 얼마나 정확하고 오류가 적게 발생하는지에 기반합니다. 하지만, 단순하게 구성하면 잘못된 평가 결과에 빠질 수 있습니다. 특히 0, 1 혹은 긍정 부정을 판단하는 **이진분류**에서는 정확도accuracy보다 다른 성능 평가 지표가 더 중요시 됩니다.


#### 2. 2. 1. 정확도(Accuracy)
\begin{equation}
정확도Accuracy = {예측 가능한 동일한 데이터 건수 \over 전체 예측 데이터 건수}
\end{equation}


* 정확도는 전체 예측 데이터 건수 중 예측 결과가 동일한 데이터 건수로 계산
* scikit-learn에서는 `accuracy_score` 함수를 제공


정확도는 직관적인 모델 예측 성능을 나타내는 평가 지표입니다. 하지만 이진분류의 경우 데이터 구성에 따라 모델 성능을 왜곡할 수 있습니다. 타이타닉 데이터에서 예측 정확도를 보면 보통 80%가 나오는데요. 여성 대 남성 생존 비율도 약 80%이기 때문에 알고리즘 적용 없이 여성이면 생존했다고 분류해도 비슷한 수치가 나올 수 있습니다. 즉, 성별이라는 이진 조건만 가지고도 높은 정확도를 만들 수 있습니다.




```python
from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2
```


#### 2. 2. 2. 오차 행렬(Confusion Matrix)




하지만 성별만으로 정확도가 높은 분류 모델은 과연 좋은 모델일까요? 자료나 모델이 편향bias되어 있지는 않을까요? 그래서 우리는 예측값과 실제 값을 두고 얼마나 정확하게 예측했는지 알 수 있는 혼동행렬Confusion Matrix를 사용합니다.


||Positive|Negative|
|---|:---:|:---:|
|**Positive**|TP|FN|
|**Negative**|FP|TN|


행row는 실제 값이고, 열column은 예측 값입니다.


+ **True / False** : 예측값과 실제값이 같은가 /틀린가


뒷글자를 결정하는 기준은 예측값이 기준이며 예측이 맞았는가를 기준으로 앞글자에 True와 False


+ TP, FN, FP, TN
    * **True Negative**: 예측값을 Negative 값 0으로 예측했고, 실제 값도 Negative 값 0(정답)
    * **False Positive**: 예측값을 Positive 값 1로 예측했는데, 실제 값은 Negative 값 0(오답)
    * **False Negative**: 예측값을 Negative 값 0으로 예측했는데, 실제 값은 Positive 값 1(오답)
    * **True Positive**: 예측값을 Positive 값 1로 예측했고, 실제 값도 Positive 값 1(정답)


갑자기 현기증이 나는 것 같죠? 하지만 이 값들로 만들어진 지표가 정확도Accuracy, 정밀도Precision, 재현율Recall, 오류율Error Rate 등이니 한 번 짚고 넘어갑시다:)




+ 정확도Accuracy, 정밀도Precision, 재현율Recall, 오류율Error Rate
    * **정확도Accuracy** = (TN + TP) / (TN + FP + FN + TP) = (맞다고 예측하고 실제로 예측이 맞은 것) / (전체 예측)
    * **정밀도Precision** = TP / (FP + TP) = (맞다고 예측하고 실제로 맞은 것) / (맞다고 예측한 것)
    * **재현율Recall** = TP / (FN + TP) = (맞다고 예측하고 실제로 맞은 것) / (오답으로 예측한 것)
    * **오류율Error Rate** = (FN + FP) / (TN + FP + FN + TP) = (T/F 모두 맞추지 못한 것) / (전체 예측)


#### 2. 2. 3. **정밀도Precision, 재현율Recall, 특이도Specificity, 오경보율FPR, F1 score**




전체 예측한 것 중에 예측이 맞은 것이 정확도accuracy에 해당합니다. 


하지만, 예를 들어 암이 양성인 경우가 positive로 1, 음성인 경우 negative로 0인 이진데이터를 분류한다고 합시다. 다른 지표들이 암에 끼치는 영향을 예측한다고 할 때, 그 데이터에는 암에 positve한  1값이 많을 수 밖에 없습니다. 그러면 다른 조건들이 무시되고 어떤 식으로 해도 positive가 나올 겁니다. **이진 분류에 오차행렬이 사용되는 이유는 데이터셋이 불균형할 가능성이 높기 때문**입니다.


아니 그러면 오차행렬은 왜 계산한 것이냐고 묻고 싶을 수 있습니다. 여기 나온 TN, FP, NF, TP를 잘 조합해 **정밀도Precision**와 **재현율Recall**, 특이도, 오경보율, F**1 score, ROC curve(AUC)** 등을 구해 모델의 성능을 평가할 수 있습니다.


+ **정밀도Precision와 재현율Recall**


![precision_recall_img](https://www.notion.so/image/https%3A%2F%2Fmblogthumb-phinf.pstatic.net%2FMjAxOTAzMjZfMTMy%2FMDAxNTUzNTYzMjEyNzY3.r7a9DpR17aQ2kwvBlUSPyrWlfDBq3HjEbq0jLmilEe4g.bAZGz___y08RyaYs3DmhGMfjchubzSbJn8aIpaJSYGUg.PNG.qbxlvnf11%2F525px-Precisionrecall.svg.png%3Ftype%3Dw800?table=block&id=5ad71aea-9600-4c7e-abb3-71f7a462631d&spaceId=7bea9ff0-f60d-4aa4-8685-ca87dc14b6ce&userId=d945f1de-10cf-4b31-bddd-9736aa1c3f3a&cache=v2)


**1. 정밀도Precision** = **TP/(FP+TP)**: 예측을 Positive(1) 로 한 대상 중에 예측과 실제 값이 Positive(1)로 일치한 데이터의 비율 (precision_score())


쉽게 말해 **모델이 찾은 1(positive) 중에 실제로 1이 얼마나 존재하느냐**를 의미합니다. 진짜를 진짜라고 제대로 분류한 비율. 0에서 1 사이의 값을 가지고 1에 가까울수록 좋은 모델입니다.


ex. 스팸메일: 스팸 메일로 찾은 것(FP스팸이라고 찾았는데 아닌 것 + TP스팸이라고 찾았는데 진짜 스팸인 것) 중에 진짜 스팸메일인 것(TP)


- TP : 스팸 메일을 스팸으로 예측한 경우
- FN : 스팸 메일을 정상으로 예측한 경우
- FP : 정상 메일을 스팸으로 예측한 경우
- TN : 정상 메일을 정상으로 예측한 경우


TP와 TN은 정확히 예측한 경우이고 FP와 FN은 잘못 예측한 경우입니다.하지만 이 모델에서는 FP에서 정상 메일을 스팸 메일로 예측했을 때 정상적인 업무가 불가능하게 됩니다.


**정밀도Precision는 미처 잡아내지 못하는 것이 있더라도 더 정확한 예측이 필요할 때 사용합니다.**


정리: 예측이 편향되어 있으므로, 정밀도가 높은데, 다르게 말하면 재현율이 낮다는 의미. 그래서 재현율의 성능에 초점을 맞춰서 해보자!


- 재현율Recall - **TPR**(TruePositiveRate)




**2. 재현율Recall** = **TP/(FN+TP)**: 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율 (recall_score()) 
    
재현율은 민감도(Sensitivity)라고도 합니다. **얼마나 우리가 예측하는 값을 빠뜨리지 않고 예측했는지(찾아야 할 것중에 실제로 찾은 비율)**를 나타내며, 실제로 Positive인 것들 중 얼마나 많이 맞추었는가를 의미합니다. 0에서 1 사이의 값을 가지며 1에 가까울수록 좋은 모델입니다.
    
e.g.) 암 환자: 실제 암 환자를 양성이 아닌 음성으로 잘못 판단하면 생명을 앗아갈 정도의 심각한 문제(FN)
    
e.g.) 금융거래 사기: 실제 사기 건을 아니라고 잘못 판단하면 회사에 업무적으로 미치는 손해가 큼(FN)
    
e.g.) 코로나 판단 모델
    
    - TP : 코로나 환자에게 코로나라고 예측한 경우
    - FN : 코로나 환자에게 코로나가 아니라고 예측한 경우
    - FP : 코로나 환자가 아닌 사람에게 코로나라고 예측한 경우
    - TN : 코로나 환자가 아닌 사람에게 코로나가 아니라고 예측한 경우
    
여기서 TP와 TN은 정확히 예측한 경우이고 FP와 FN은 잘못 예측한 경우입니다. 하지만 FN에서 코로나 환자에게 코로나가 아니라고 예측했을 때 최악의 문제가 생기게 됩니다.
    
**Recall(재현율)은 잘못 걸러내는 비율이 높더라도 놓치는 것이 없도록 하는 경우에 사용**합니다.








3. **특이도Specificity**
특이도specificity/TNR(true negative Rate): negative(0)중에 얼마나 0을 예측했는지. 0과 1 사이의 값을 가지며 1에 가까울수록 좋은 모델
    \begin{equation}
    \frac{\text{TN}}{\text{TN}+\text{FP}}
    \end{equation}
    
4. **오경보율FPR(FalsePositiveRate)**: 찾고자 하는 것이 아닌 0에 대해 1이라고 **얼마나 잘못 예측 했는지를 나타내는 지표**. 0에 가까울수록 좋은 모델. 민감도가 커지면 FP가 커질 수 있으며 오경보율은 커지게 됩니다. **이 관계를 곡선으로 나타낸 것이 아래 서술할 ROC curve**입니다.
    \begin{equation}
    \frac{\text{FP}}{\text{FP}+\text{TN}} = 1-TNR
    \end{equation}






5. **F1 Score(F-measure)**
* 정밀도와 재현율을 결합한 지표
* 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 높은 값을 가짐

\begin{equation}
F1 = 2 \times \frac{precision \times recall}{precision + recall}
\end{equation}

가장 좋은 지표는 정확도precision과 재현율recall이 모두 높은 경우입니다. 하지만 정확도와 재현율은 Trade-off관계이기 때문에 하나가 높아지면 다른 하나가 낮아집니다. 그래서 이 둘을 잘 조합한 것이 F1 score입니다.


![f1_img](https://www.notion.so/image/https%3A%2F%2Fmedia.vlpt.us%2Fimages%2Fyepark%2Fpost%2F361d4cb9-e9f7-4e23-aa1a-7937d7d534e9%2F%25ED%258A%25B8%25EB%25A0%2588%25EC%259D%25B4%25EB%2593%259C%25EC%2598%25A4%25ED%2594%2584.PNG?table=block&id=5832dce2-2148-468c-b6f3-4eacdd2b5806&spaceId=7bea9ff0-f60d-4aa4-8685-ca87dc14b6ce&width=960&userId=d945f1de-10cf-4b31-bddd-9736aa1c3f3a&cache=v2)


이 그래프의 경우 임계값threshold이 0.45 쯤일 때 베스트네요.


F1 score = 정밀도와 재현율을 결합한 지표. 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 형태일때 높은 값. 정밀도와 재현율의 조화평균입니다.







#### 2. 2. 4. ROC 곡선과 AUC


결국 모형이 좋다는 말의 뜻을 생각해보면, 모든 환자에게 양성 판정을 내리고**TPR(재현율recall)**, 모든 정상인에게 음성 판정**FPR(오경보율)**을 내리면 완벽합니다. 


만약 모든 진단에 대해 양성 판정을 내리면 어떻게 될까요? 이 경우 병에 걸린 모든 환자를 찾을 수 있습니다. 왜냐면 모든 사람이 양성 판정을 받기 때문이죠. 하지만 정상인도 환자로 판정 받는 다는 단점이 있습니다. 


그럼 모든 진단에 대해 음성 판정을 내리면 어떻게 될까요? 이 경우에는 모든 정상인에 대해 올바른 판정이 내려집니다. 하지만 병에 걸린 환자 또한 정상인이라고 판정 받으므로 병을 치료할 수 없습니다.


ROC 커브는 이러한 다양한 부분을 한눈에 볼 수 있는 판정법입니다. Figure1에서 보면, 병에 걸린 사람을 양성 판정하고, 정상인을 정상인이라 판정하는 가장 이상적인 판정, 즉, TPR = 1 이고, FPR = 0 인 경우가 가장 이상적입니다.(Perfect Classification)


ROC curve는 분류에 대한 성능 평가라기보다 회귀처럼 **어떤 값을 표현한 후 문턱값(threshold)를 기준으로 0과 1을 나누는 경우에 그 값이 얼마나 0과 1에 대해 잘 나눌 수 있는 값인지 측정**합니다.


* ROC 곡선은 FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지 나타내는 곡선
  + TPR(True Positive Rate): TP / (FN + TP), 재현율
  + TNR(True Negative Rate): TN / (FP + TN)
  + FPR(False Positive Rate): FP / (FP + TN), 1 - TNR


* AUC(Area Under Curve) 값은 ROC 곡선 밑에 면적을 구한 값 (1이 가까울수록 좋은 값)


**ROC curve에서 짚고 넘어가야 할 것.**


1. True Positive Rate과 False Positive Rate: Recall과 오경보율
    
True Positive**TP**는 실제로 이 사람은 암에 걸려있고, 판단을 암에 걸렸다고 판단한 것이고, False Positive**FP**는 실제로는 암에 걸리지 않았는데도 암에 걸렸다고 ‘잘못’ 판단한 것을 의미합니다.
    
![https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic3.png](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic3.png)
    
- **Threshold가 낮은 경우**: 모든 환자들을 다 암환자라고 판단한다면 실제로 암이 걸린 환자들도 모두 암 환자로 판정되면서 **True Positive Rate**가 높아집니다.(주황색 면)
        
동시에 암이 걸리지 않았던 환자들도 모두 암 환자로 판정됩니다. 즉, **False Positive Rate도 동시에 높아**집니다.(파란색 면)
        
    
이때, threshold임계값을 모두 환자로 판정한다는 것은 위의 그림에 0에 있는 threshold가 낮다는 의미입니다.
    
![https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic4.gif](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic4.gif)
    
- **Threshold가 높은 경우**: 반대로 모든 환자들이 암환자가 아니라고 판단한다고 합시다. 암이 걸리지 않은 환자 뿐만 아니라 암에 걸린 환자도 모두 정상인으로 판정하면서 **True Positive Rate과 False Positive Rate 모두 낮아집니다**.
    
![https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic5.gif](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic5.gif)
    
**threshold가 변하면서 False Positive Rate(FPR)과 True Positive Rate(TPR)의 값이 바뀐다는 것을 알 수 있습니다.**
    
**또, threshold가 높아지건 낮아지건 FPR과 TPR은 어느정도는 비례적으로 함께 커지거나 작아졌습니다.**
    
2. **ROC Curve위의 한 점이 의미하는 것은 무엇인가?**
    
현 위의 점은 가능한 모든 threshold별 FPR과 TPR을 알아보겠다는 의미 입니다.
    
즉, 현재 이진 분류기의 분류 성능은 변하지 않되, 가능한 모든 **threshold별 FPR과 TPR의 비율을** 의미합니다.
    
![https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic6.gif](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic6.gif)
    
3. **ROC Curve의 휜 정도가 의미하는 것은 무엇인가?**
    
아래의 그림에서 볼 수 있듯이 두 **클래스를 더 잘 구별할 수 있다면 ROC 커브는 좌상단에 더 가까워지게 됩니다.** 즉, 좌상단일수록 성능이 좋습니다.
    
완벽하게 구별한다면 모든 FPR도 1이고, TPR은 1일겁니다.
    
![https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic7.gif](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-05-ROC/pic7.gif)
    
- **AUC(Area Under the Curve)**
    
AUC는 아까 만든 ROC curve에서 아래쪽 면적을 의미합니다. (적분 값)
    
![https://developers.google.com/machine-learning/crash-course/images/AUC.svg?hl=ko](https://developers.google.com/machine-learning/crash-course/images/AUC.svg?hl=ko)
    
1. **auc=1**
    - 두 곡선이 전혀 겹치지 않는 경우.
    - TN과 TP를 완벽하게 잡아낸 경우. 이상적인 분류.
        
2. **auc=0.7**
    - threshold에 따라 오류를 최소, 혹은 최대화 할 수 있다.
    - auc=0.7: 분류 모델이 TN, TP를 구별할 수 있는 확률이 70%임을 의미한다.
3. **auc=0.5**
    - 분류가 제대로 되지 않는 경우.
    - TN, TP를 제대로 구별하지 못한다.
4. **auc=0**
    - 분류가 정반대로 되고 있는 경우.
    
[AUC-ROC 커브](https://bioinformaticsandme.tistory.com/328)


여기까지 분류 모델의 평가지표에 대해 알아봤습니다. 헷갈리시시죠? 분석의 의미를 파악하기 위해 꼭 필요한 과정이니 프로젝트 시작, 중간, 마지막에 틈틈이 확인하시기 바랍니다.