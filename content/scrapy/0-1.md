---
title: "추천 학습 가이드"
slug: 0-1
category: "0. 추천 학습 가이드"
---

코사다마의 웹크롤링 심화 스터디에 참여하신 모든 분들을 환영합니다 🙌


## 스터디 목표

본 커리큘럼은 웹크롤링 심화 과정으로 기존 데이터 사이언스 입문에서 배웠던 BeautifulSoup나 Selenium보다 상당히 복잡한, **Scrapy**라는 새로운 크롤링 프레임워크를 다루려고 합니다.

Scrapy는 빠른 크롤링 속도와 넓은 확장성, 뛰어난 데이터 처리와 저장 능력을 가지고 있습니다. 이러한 장점 덕분에 이미 해외에서는 자주 쓰이고 있는 고급 기법이죠 ✨

Scrapy는 **터미널 환경**에서 크롤링이 이루어지기 때문에 처음 배우는 입장에서는 상당히 어려울 수 있습니다. 그래서 이번 커리큘럼에서는 Scrapy의 모든 부분을 다루기 보다는, **기본적인 사용법을 익히는 것을 목표**로 합니다. 


## 스터디 진행환경

Scrapy는 **터미널 환경**에서 작업합니다. 아래 영상은 본 커리큘럼을 이해하는 데 도움이 되니 시청 바랍니다.

- [디렉터와 터미널 - 개발자가 반드시 알아야 할 기본소양](https://www.youtube.com/watch?v=6z7FVYXnk3E&feature=youtu.be)


## 스터디 유의사항

- 커리큘럼은 다음 자료를 참고하여 만들어졌습니다.
    1. 잔재미코딩, **『현존 최강 크롤링 기술: Scrapy와 Selenium 정복』**, (온라인강의, ****인프런)
    2. 카토 코타, **『파이썬을 이용한 웹 크롤링과 스크레이핑』**, 위키북스(2018), 80-83, 131-133, 267-320
    3. 라이언 미첼, **『파이썬으로 웹 크롤러 만들기(2판)』**, 한빛미디어(2019), 95-114
    
    책은 구매하실 필요 없습니다. 다만 웹크롤링에 대한 책을 하나 구비하고 싶다면 *카토 코타의 '파이썬을 이용한 웹 크롤링과 스크레이핑(위키북스)'*를 추천드립니다.
    
- 과제 미제출 시 아웃카운트 1점, 지각제출은 0.5점이 적립됩니다. 아웃카운트가 총 3점이 적립된 경우 **더 이상 스터디에 참여할 수 없습니다.** 스터디의 원활한 진행을 위해 부득이하게 운영진들이 내린 결정이니 이해해 주시면 감사하겠습니다. 열심히 교안을 만든 운영진을 봐서라도 성실한 과제 수행 부탁해요,,, 🙃


## 추천 학습 양
<table> 
<thead> 
<tr>  
<th>주차</th> 
<th>학습 대주제</th>  
<th>학습 소주제</th>  
</tr>  
</thead> 
<tbody>  
<tr> 
<td rowspan=2>1주차</td>  
<td>1. Scrapy 소개</td> 
<td> 
1-1. Scrapy란?<br> 
1-2. Scrapy는 객체 지향 프로그래밍이다!<br>
1-3. Scrapy 설치하기<br>
</td> 
</tr> 
<tr> 
<td>2. Scrapy shell</td> 
<td> 
2-1. Scrapy shell이란?<br> 
2-2. Scrapy shell에서 CSS Selector와 XPath로 데이터 가져오기<br>
</td>
</tr>  
<tr> 
<td>3. Scrapy 프로젝트 생성</td> 
<td> 
3-1. Scrapy 프로젝트 생성<br> 
</td>
</tr>      
<tr>  
<td rowspan=2>2주차</td> 
<td>4. Spider(크롤러) 만들기</td> 
<td> 
4-1. Spider 클래스<br> 
4-2. Robots.txt(로봇 배제 표준)<br>
</td>
</tr>  
<tr>  
<td>5. 크롤링 데이터 다루기</td>
<td> 
5-1. Item 만들기<br> 
5-2. Spider 수정하기<br>
5-3. 데이터 저장하기<br>
5-4. 데이터 후처리하기<br>
</td>
</tr>  
<tr>  
<td rowspan=2>3주차</td> 
<td>6. 준비</td> 
<td> 
6-1. 프로젝트, Spider(=크롤러) 만들기<br> 
6-2. Item 만들기 <br>
6-3. Request 메서드, callback과 meta 파라미터 <br>
</td>
</tr>  
<tr>  
<td>7. 실습</td>
<td> 
7-1. 메인 카테고리의 베스트 상품 크롤링 <br> 
7-2. 메인 카테고리, 서브 카테고리의 베스트 상품 크롤링<br>
7-3. 데이터 후처리 및 저장
</td>
</tr>  
<tr>  
<td rowspan=2>4주차</td> 
<td>8. 실습 환경 준비</td> 
<td> 
8-1. VS Code 설치하기<br> 
8-2. VS Code 사용 팁⭐ <br>
8-3. VS Code에서 작업하기 <br>
</td>
</tr>  
<tr>  
<td>9. Scrapy LOG_LEVEL</td>
<td> 
9-1. Scrapy LOG_LEVEL  <br> 
</td>
</tr>  
</tbody> 
</table>
