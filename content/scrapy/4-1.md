---
title: 'Spider 클래스'
slug: 4-1
category: '4. Spider(크롤러) 만들기'
---
이번 주차에서는 11번가 사이트 크롤링을 통해 Scrapy 프로젝트 파일을 하나하나 파헤쳐 보겠습니다.

1주차에서도 언급했지만 클래스에 대한 내용 복습은 필수입니다.

클래스, 객체, 메서드 등 용어를 중심으로 복습해 주세요 🙂

- [[점프 투 파이썬] 05-1 클래스](https://wikidocs.net/28)

<br>

##### 📢 **공지사항**


---

3주차까지는 py 파일 수정을 **jupyter notebook**으로 합니다.

원래는 텍스트 에디터(코드 편집 프로그램)를 사용해야 되는데, 일단은 Scrapy 학습에 초점을 두기로 했어요. (주피터 노트북은 텍스트 에디터가 아닙니다!!!)

대표적인 텍스트 에디터로는 Visual Studio Code(VS Code), Sublime Text, Atom 등이 있습니다. 4주차에 VS Code의 기초적인 사용법을 익혀볼 예정이니 기대해 주세요 😊

- **📖 살펴보기: 텍스트 에디터 종류**
  
    ![0](./images/WEEK2/0.png)
    
    - [Code Time Data: Ranking the Top 5 Code Editors in 2019 - /src/ blog](https://www.software.com/src/ranking-the-top-5-code-editors-2019)
    
    - [[코딩] 프로그래밍을 위한 텍스트 편집기 4종 추천](https://oriyong.tistory.com/64)

<br>

<br>

시작하기 앞서 jupyter notebook으로 **settings.py**를 열어 한 가지 설정을 추가하겠습니다.

- **settings.py** 여는 법을 모른다면...
    1. jupyter notebook을 실행합니다.
       
        ![1](./images/WEEK2/1.png)
        
    2. 자신이 Scrapy 프로젝트를 만든 폴더로 이동합니다. (예: Downloads > Scrapy)
       
        ![2](./images/WEEK2/2.png)
        
        ![3](./images/WEEK2/3.png)
        
    3. **settings.py**를 찾아 클릭합니다.
       
        ![4](./images/WEEK2/4.png)
    

<br>

```python
DOWNLOAD_DELAY = 1    # 페이지 다운로드 간격을 1초로 지정
```

![5](./images/WEEK2/5.png)

<br>

이 설정을 해주지 않으면 페이지 다운로드 간격이 0초라 웹사이트에 부하를 걸게 될 수 있습니다. 사이트 관리자의 워라밸을 위해 해당 설정을 꼭 저장해 줍시다 👨‍💻

<br>

## 1. Spider(크롤러) 만들기

![6](./images/WEEK2/6.png)

먼저 [11번가 사이트의 베스트 카테고리 - 전체 부문](https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&xfrom=main^gnb)를 크롤링하는 spider(=크롤러)를 만들어보겠습니다.

```powershell
#명령어 - 터미널에 입력
scrapy genspider <spider 이름> "크롤링 페이지 주소"

# 예: 11번가 베스트 카테고리 spider 생성
# 반드시 경로 이동한 뒤!!! spider 만들어 주세요!!!!!! 
cd Downloads\Scrapy\st11    # 본인의 프로젝트 디렉토리로 이동
scrapy genspider st11_all "www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain"
```

![7](./images/WEEK2/7.png)

(저는 이미 만들어진 상태라 출력되는 메세지가 달라요. **'create spider '크롤러 이름' using template 'basic' in module:'** 이 출력되면 성공!)

- spider 이름: st11_all
  
    프로젝트명과 동일하게 spider 이름을 지을 수 없음.
    (Cannot create a spider with the same name as your project)
    
- 크롤링 페이지 주소: 베스트 카테고리 - 전체 부문
  
    [https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain](https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain)
    
    (크롤링 페이지 주소에서 https:// 는 빼줍니다. 이유는 하단에서 설명드릴게요.)

<br>

![8](./images/WEEK2/8.png)

spiders 디렉토리 안에 **st11_all.py**가 생성된 것을 확인할 수 있습니다.

<br>

### 1-1. Spider 클래스

---

jupyter notebook으로 **st11_all.py**를 열어보면 다음과 같습니다.

![9](./images/WEEK2/9.png)

Scrapy의 spider는 **scrapy.Spider**를 상속받는 클래스입니다. 클래스 이름은 자유롭게 수정해도 되지만, 반드시 **scrapy.Spider**를 상속받아야 합니다.

클래스에 대한 내용이 기억나지 않는다면 다음 자료를 **반드시 복습**해주세요.

- [파기문 6주차](https://www.notion.so/2021-PYTHON-BASIC-GRAMMAR-WEEK6-80843dca0b5b4032a5025c03fe9fdb41)
- [[점프 투 파이썬] 05-1 클래스](https://wikidocs.net/28#_9)

<br>

**St11AllSpider 클래스**에는 3가지 속성(**name**, **allowed_domains**, **start_urls**)과 **parse() 메서드**가 있습니다.

1. **name**
   : spider의 이름을 설정하는 속성
   
    scrapy genspider 명령어를 입력했을 때 자동으로 입력된 것이며, 이후 spider 실행에 입력된 이름이 사용됩니다.
   
2. **allowed_domains**
   : 크롤링 페이지 주소의 리스트를 지정하는 속성
   
    마구잡이로 링크를 이동하다 보면 예상하지 못한 웹페이지에 접근하는 경우가 발생합니다. 이런 경우를 막고자 허용된 주소 이외에는 크롤링 하지 못하도록 하는 속성입니다.
   
    저희 실습에서는 삭제해도 무관하나, 불특정 다수의 웹사이트를 대상으로 자유롭게 크롤링하는 경우를 제외하면 지정하는 것을 권장합니다.
   
    ```python
    # 예
    allowed_domains = ['www.11st.co.kr']
    # 11번가 내의 여러 페이지를 크롤링하고자 한다면 아래처럼 입력
    # 의미: allowed_domains로 시작하는 url이 아니면 접근하지 말 것
    ```
   
3. **start_urls
   :** 크롤링할 페이지 주소를 나타내는 속성
   
    주소 목록을 리스트 혹은 튜플 형식으로 지정합니다. 저희는 11번가 베스트 카테고리 하나만 입력하지만 여러 개의 주소를 지정할 수 있습니다.
   
    ```python
    # 예
    # 대한민국 쇼핑몰 사이트를 크롤링하고자 하면
    start_urls = ['http://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain',
    'https://www.gmarket.co.kr', 'http://www.auction.co.kr', 'https://www.coupang.com' 
    ]
    ```
   
    위에서 spider를 만들 때 크롤링 페이지 주소를 https:// 를 제외하고 입력했는데요. 그 이유는 start_urls 에 기본적으로 http:// 가 입력되어 있어 https:// 가 중복 기입되기 때문입니다.
   
    ```python
    # 예
    # 주소 끝의 '/(슬래시)' 지워줄 것!
    start_urls = ['[http://https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain](http://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain/)']
    ```
   
4. **parse()
   :** 추출한 웹페이지 처리를 위한 콜백 함수
   
    response에 **start_urls**에 지정된 주소의 서버에서 넘겨받은 응답 데이터가 크롤링 결과로 담기기 때문에, response를 반드시 인자로 받아야 합니다.
   
   #####  💡 **개념잡기: 콜백 함수(callback function)란?**
   
   
   ---
   
    - 관련 영상(~2:27까지 시청): [https://youtu.be/3NVB9Y3jXRM](https://youtu.be/3NVB9Y3jXRM)
      - 요약: 사용자에 의해 직접 호출되는 것이 아닌 다른 함수에서 호출되는 함수
   

<br>

response에 응답 데이터가 제대로 담기는지 확인해봅시다. **st11_all.py**를 살짝 수정해 주세요.

```python
#st11_all.py
def parse(self, response):
    #pass
    print(response.text)    # response.text에 크롤링 된 데이터가 담겨있음
```

![10](./images/WEEK2/10.png)

<br>

spider를 실행해봅니다.

```powershell
#명령어 - 터미널에 입력
scrapy crawl <spider 이름>

# 예: 11번가 베스트 카테고리 크롤링 
scrapy crawl st11_all
```

<br>

![11](./images/WEEK2/11.png)

출력된 메세지를 잘 살펴보면 '**DEBUG: Forbidden by robots.txt:**' 라는 문장을 찾을 수 있습니다.

웹페이지 정보도 크롤링되지 않은 것 같아요. 이유가 무엇일까요?
