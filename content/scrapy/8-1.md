---
title: 'Scrapy Log_LEVELì„ í†µí•œ ì˜¤ë¥˜ ì°¾ê¸°'
slug: 8-1
category: '8. Scrapy Logging'
---
Scrapyë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ ì¶œë ¥ë˜ëŠ” ë¡œê·¸ëŠ” ì²˜ìŒì—ëŠ” ì‹ ê¸°í•˜ì§€ë§Œ, ì¥í™©í•œ íƒ“ì— ì˜¤ë¥˜ ë©”ì„¸ì§€ë¥¼ ì°¾ê¸° í˜ë“¤ê²Œ í•©ë‹ˆë‹¤.

- ìˆœê°„í¬ì°© ì˜¤ë¥˜ ë©”ì„¸ì§€ ì°¾ê¸° ğŸ”

  [ìˆœê°„í¬ì°© ì˜¤ë¥˜ ë©”ì‹œì§€ ì°¾ê¸°.mp4](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0befac4c-ae57-49cd-af59-58762143a05b/%EC%88%A8%EC%9D%80_%EC%98%A4%EB%A5%98_%EC%B0%BE%EA%B8%B0.mp4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220304%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220304T185428Z&X-Amz-Expires=86400&X-Amz-Signature=94ea8cc77c298d75ffd0cad4655c1624e8fbbdc25c48aedb92bff295d4ea3ced&X-Amz-SignedHeaders=host&x-id=GetObject)

ì´ëŸ¬í•œ ê³ ë¯¼ì€ **settings.py**ì— LOG_LEVEL ì„¤ì •ì„ ì¶”ê°€í•´ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ **ê°œë…ì¡ê¸°: LOG_LEVEL ì„¤ì •**
> LOG_LEVELì€ í‘œì‹œí•  ë©”ì„¸ì§€ì˜ ìˆ˜ì¤€ì„ ê²°ì •í•©ë‹ˆë‹¤. Scrapyì˜ í‘œì¤€ì ì¸ LOG_LEVELì€ ì´ 5ê°€ì§€ì…ë‹ˆë‹¤.
>
> 1. CRITICAL â€” ìµœê³  ì‹¬ê°ë„
> 2. ERROR â€” ì¶”ì²œ
> 3. WARNING
> 4. INFO
> 5. DEBUG â€” ìµœì € ì‹¬ê°ë„, ê¸°ë³¸ ì„¤ì •ê³¼ ë™ì¼
> 
> ë¡œê·¸ ìˆ˜ì¤€ì„ WARNINGìœ¼ë¡œ ì„¤ì •í•˜ë©´ CRITICAL, ERROR, WARNING ë¡œê·¸ë§Œ ì¶œë ¥ë˜ê³ , DEBUGë¡œ ì„¤ì •í•˜ë©´ ëª¨ë“  ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. ìˆ˜ì¤€ì„ ë³€ê²½í•˜ë©´ì„œ ì°¨ì´ë¥¼ íŒŒì•…í•´ë³´ì„¸ìš”.

ğŸ“– **ì°¸ê³ í•˜ê¸°: ë¡œê¹…(Logging)ì´ë€?**
> ë¡œê¹…ì´ë€ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ëŠ” í–‰ìœ„ë¥¼ ë§í•©ë‹ˆë‹¤.
> 
> í”„ë¡œê·¸ë˜ë°ì—ì„œ ë¡œê·¸(Log)ëŠ” ìš´ì˜ì²´ì œë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ê°€ ì‹¤í–‰í•˜ëŠ” ë„ì¤‘ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸, í˜¹ì€ ê°ê¸° ë‹¤ë¥¸ ì‚¬ìš©ìì˜ í†µì‹  ì†Œí”„íŠ¸ì›¨ì–´ ê°„ì˜ ë©”ì„¸ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
> 
> ì¶œì²˜: [[ìœ„í‚¤ë°±ê³¼] ë¡œê·¸íŒŒì¼](https://ko.wikipedia.org/wiki/%EB%A1%9C%EA%B7%B8%ED%8C%8C%EC%9D%BC)


```python
#settings.py
LOG_LEVEL = 'ERROR'    # ì§ì ‘ ì…ë ¥
```

![0](/scrapy/8-1/0.png)

- [í¸ì•ˆí•´ì§„ ì˜¤ë¥˜ ì°¾ê¸°.mp4](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/99b99a64-6a22-487a-a014-29c150485314/%ED%8E%B8%EC%95%88%ED%95%B4%EC%A7%84_%EC%98%A4%EB%A5%98_%EC%B0%BE%EA%B8%B0.mp4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220304%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220304T181907Z&X-Amz-Expires=86400&X-Amz-Signature=d184fa7eb668dd10d50e35218ba62393c62378afce39bf697d6c5d182dcff3e6&X-Amz-SignedHeaders=host&x-id=GetObject)

ì˜¤ë¥˜ ë©”ì„¸ì§€ë¥¼ ì°¾ê¸° ë§¤ìš° ì‰¬ì›Œì¡Œë„¤ìš” ğŸ˜†

ì´ì™¸ì—ë„ ì¶œë ¥ë˜ëŠ” printë¬¸ì„ ë³´ê³  ì–´ëŠ ë¶€ë¶„ì´ ì˜ëª»ëëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 3ì£¼ì°¨ì˜ **st11_best.py**ì— ì‘ì„±í–ˆë˜ printë¬¸ 3ê°œë¥¼ ê¸°ì–µí•˜ì‹œë‚˜ìš”?

- ê¸°ì–µì´ ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´...
  
    ```python
    #st11_best.py: ìµœì¢…
    import scrapy
    from st11.items import St11Item
    
    class St11BestSpider(scrapy.Spider):
        name = 'st11_best'
    
        def start_requests(self):
            yield scrapy.Request(url="https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo=0",
                                 callback=self.parse_mainpages)
            
        def parse_mainpages(self, response):
            print("parse_mainpages")
            category_names = response.css(quiz:ë©”ì¸ ì¹´í…Œê³ ë¦¬ëª…ì˜ css ì„ íƒì ê²½ë¡œ).getall()
            for idx, name in enumerate(category_names):
                yield scrapy.Request(url="https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo="+str(idx), 
                                     callback=self.parse_items, 
                                     meta={'maincategory_name':category_names[idx], 'subcategory_name':'All'})
            for idx, name in enumerate(category_names):
                yield scrapy.Request(url="https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo="+str(idx), 
                                     callback=self.parse_subcategory, 
                                     meta={'maincategory_name':category_names[idx],'index':idx}
                                    )
        
        def parse_subcategory(self, response):
            print('parse_subcategory', response.meta['maincategory_name'])        
            subcategory_names = response.css(quiz:ì„œë¸Œ ì¹´í…Œê³ ë¦¬ëª…ì˜ css ì„ íƒì ê²½ë¡œ).getall()
            subcategory_lists = response.css('div.sub_category_box li a::attr("onclick")').re('\(.*\)')
            subcategory_idcs = []
            for i in subcategory_lists:
                if i[2] == ',':
                    subcategory_idcs.append((int(i[1]),int(i[3:-1])))
                else:
                    subcategory_idcs.append((int(i[1:3]),int(i[4:-1])))
            
            for idx, sub in enumerate(subcategory_idcs):
                if sub[0] == response.meta['index']:
                    yield scrapy.Request(url="https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo=" + str(sub[0]) + "&dispCtgrNo=" + str(sub[1]), 
                                         callback=self.parse_items,
                                         meta={'maincategory_name':response.meta['maincategory_name'], 
                                                                          'subcategory_name':subcategory_names[idx]}
                                        )
                else:
                    continue
    
        # ë©”ì¸ ì¹´í…Œê³ ë¦¬, ì„œë¸Œ ì¹´í…Œê³ ë¦¬ì˜ ì •ë³´ë¥¼ ëª¨ë‘ í¬ë¡¤ë§í•˜ëŠ” ë©”ì„œë“œ. ì½”ë“œëŠ” ì²˜ìŒê³¼ ë™ì¼
        def parse_items(self, response):
            print('parse_items', response.meta['maincategory_name'], response.meta['subcategory_name'])
            best_items = response.css('div.viewtype.catal_ty')
            for idx, item in enumerate(best_items[1].css('li')):
                doc = St11Item()
                
                ranking = idx + 1
                title = item.css(quiz:ë² ìŠ¤íŠ¸ ìƒí’ˆì´ë¦„ì˜ css ì„ íƒì ê²½ë¡œ).get().strip()
                ori_price = item.css(quiz:í• ì¸ì „ê°€ê²©ì˜ css ì„ íƒì ê²½ë¡œ).get()
                dis_price = item.css(quiz:í• ì¸ëœê°€ê²©ì˜ css ì„ íƒì ê²½ë¡œ).get()
                
                if ori_price == None:
                    ori_price = dis_price
                ori_price = ori_price.replace(',','').replace('ì›','')
                dis_price = dis_price.replace(',','').replace('ì›','')
                
                doc['main_category_name'] = response.meta['maincategory_name']
                doc['sub_category_name'] = response.meta['subcategory_name']
                doc['ranking'] = ranking
                doc['title'] = title
                doc['ori_price'] = ori_price
                doc['dis_price'] = dis_price
         
                yield doc
    ```
    

'parse_mainpages 'ì™€ 'parse_subcategory'ì€ ì¶œë ¥ë˜ëŠ”ë° 'parse_items'ê°€ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ë©´, **parse_items ë©”ì„œë“œ**ì— ë¬¸ì œê°€ ìˆë‹¤ê³  ì¶”ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì£ . ë©”ì„œë“œì™€ ë°˜ë³µë¬¸ ì¤‘ê°„ì¤‘ê°„ printë¬¸ì„ ë„£ê³  ì‹¤í–‰í•´, ì½”ë“œë¥¼ ì˜ ì‘ì„±í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤ ğŸ‘¨â€ğŸ”§
